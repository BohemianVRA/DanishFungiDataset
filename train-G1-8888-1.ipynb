{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "062d1082-d551-43c0-a7b0-4ed1fd657481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICES=1\n",
      "env: WANDB_ENTITY=zcu_cv\n",
      "env: WANDB_PROJECT=DanishFungi2023\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICES = 1\n",
    "%env WANDB_ENTITY = zcu_cv\n",
    "%env WANDB_PROJECT = DanishFungi2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62fef7b6-9a8f-4582-889d-6f4cff185be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Sep 11 09:45:59 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A40          On   | 00000000:61:00.0 Off |                    0 |\n",
      "|  0%   44C    P0    77W / 300W |   2918MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A40          On   | 00000000:E1:00.0 Off |                    0 |\n",
      "|  0%   33C    P8    12W / 300W |      0MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    279416      C   /usr/bin/python                  2916MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1e213e5-8a30-493d-8760-bba524949867",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = os.environ[\"CUDA_DEVICES\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58e4529b-f2e8-4f7c-91d5-4048cd2480a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(script) INFO: Loading training config.\n",
      "(script) DEBUG: Extra arguments passed to the script: {}\n",
      "(script) INFO: Setting run name: vit_base_patch8_224.augreg_in21k_ft_in1k-CrossEntropyLoss-light\n",
      "(script) INFO: Using experiment directory: ./runs/vit_base_patch8_224.augreg_in21k_ft_in1k-CrossEntropyLoss-light/exp2\n",
      "(script) INFO: Using training configuration: {\n",
      "    \"augmentations\": \"light\",\n",
      "    \"image_size\": [\n",
      "        224,\n",
      "        224\n",
      "    ],\n",
      "    \"dataset\": \"DanishFungi2023-v0.1\",\n",
      "    \"architecture\": \"vit_base_patch8_224.augreg_in21k_ft_in1k\",\n",
      "    \"loss\": \"CrossEntropyLoss\",\n",
      "    \"optimizer\": \"SGD\",\n",
      "    \"scheduler\": \"cosine\",\n",
      "    \"epochs\": 100,\n",
      "    \"learning_rate\": 0.01,\n",
      "    \"batch_size\": 64,\n",
      "    \"accumulation_steps\": 4,\n",
      "    \"random_seed\": 777,\n",
      "    \"workers\": 12,\n",
      "    \"multigpu\": false,\n",
      "    \"tags\": [\n",
      "        \"Fine-tuning\",\n",
      "        \"Dino\",\n",
      "        \"DanishFungi2023\"\n",
      "    ],\n",
      "    \"root_path\": \"./\",\n",
      "    \"run_name\": \"vit_base_patch8_224.augreg_in21k_ft_in1k-CrossEntropyLoss-light\",\n",
      "    \"exp_name\": \"exp2\",\n",
      "    \"exp_path\": \"./runs/vit_base_patch8_224.augreg_in21k_ft_in1k-CrossEntropyLoss-light/exp2\"\n",
      "}\n",
      "(fgvc) INFO: Using device: cuda (1)\n",
      "(fgvc) INFO: Device names: 0: NVIDIA A40\n",
      "(script) INFO: Loading training and validation metadata.\n",
      "(script) INFO: Creating model, optimizer, and scheduler.\n",
      "(fgvc) DEBUG: Setting new prediction head with random initial weights.\n",
      "(script) INFO: Creating DataLoaders.\n",
      "(script) INFO: Creating loss function.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicekl\u001b[0m (\u001b[33mzcu_cv\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/auto/plzen4-ntis/home/picekl/Projects/DanishFungiDataset/wandb/run-20230911_115912-7f7vkpy0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mvit_base_patch8_224.augreg_in21k_ft_in1k-CrossEntropyLoss-light\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/zcu_cv/DanishFungi2023\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/zcu_cv/DanishFungi2023/runs/7f7vkpy0\u001b[0m\n",
      "(script) INFO: Training the model.\n",
      "2023-09-11 11:59:14,011 - fgvc-training - INFO - Training started.\n",
      "  0%|                                                  | 0/4162 [00:06<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"danishfungi2023_train.py\", line 247, in <module>\n",
      "    train_clf()\n",
      "  File \"danishfungi2023_train.py\", line 206, in train_clf\n",
      "    train(\n",
      "  File \"/auto/plzen4-ntis/home/picekl/Projects/FGVC/fgvc/core/training/__init__.py\", line 100, in train\n",
      "    trainer.train(\n",
      "  File \"/auto/plzen4-ntis/home/picekl/Projects/FGVC/fgvc/core/training/classification_trainer.py\", line 267, in train\n",
      "    train_output = self.train_epoch(epoch, self.trainloader)\n",
      "  File \"/auto/plzen4-ntis/home/picekl/Projects/FGVC/fgvc/core/training/classification_trainer.py\", line 161, in train_epoch\n",
      "    preds, targs, loss = self.train_batch(batch)\n",
      "  File \"/auto/plzen4-ntis/home/picekl/Projects/FGVC/fgvc/core/training/base_trainer.py\", line 85, in train_batch\n",
      "    preds = self.model(imgs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1423, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/storage/plzen4-ntis/home/picekl/.local-carnivore_id_v22_12_0.sif/lib/python3.8/site-packages/timm/models/vision_transformer.py\", line 652, in forward\n",
      "    x = self.forward_features(x)\n",
      "  File \"/storage/plzen4-ntis/home/picekl/.local-carnivore_id_v22_12_0.sif/lib/python3.8/site-packages/timm/models/vision_transformer.py\", line 640, in forward_features\n",
      "    x = self.blocks(x)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1423, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py\", line 204, in forward\n",
      "    input = module(input)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1423, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/storage/plzen4-ntis/home/picekl/.local-carnivore_id_v22_12_0.sif/lib/python3.8/site-packages/timm/models/vision_transformer.py\", line 155, in forward\n",
      "    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1423, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/storage/plzen4-ntis/home/picekl/.local-carnivore_id_v22_12_0.sif/lib/python3.8/site-packages/timm/models/vision_transformer.py\", line 92, in forward\n",
      "    attn = q @ k.transpose(-2, -1)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.76 GiB (GPU 0; 44.39 GiB total capacity; 42.18 GiB already allocated; 1.45 GiB free; 42.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mvit_base_patch8_224.augreg_in21k_ft_in1k-CrossEntropyLoss-light\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/zcu_cv/DanishFungi2023/runs/7f7vkpy0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230911_115912-7f7vkpy0/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python danishfungi2023_train.py \\\n",
    "    --config-path configs/vit_base_patch8_224.augreg_in21k_ft_in1k.yaml \\\n",
    "    --cuda-devices $CUDA_DEVICES \\\n",
    "    --wandb-entity $WANDB_ENTITY \\\n",
    "    --wandb-project $WANDB_PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859f8dda-5c9a-4698-8117-4c01b0850d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
