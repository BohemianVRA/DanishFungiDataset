{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import json\n",
    "import tqdm\n",
    "import time\n",
    "import timm\n",
    "import torch\n",
    "import random\n",
    "import sklearn.metrics\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import Adam, SGD\n",
    "from scipy.special import softmax\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from albumentations import Compose, Normalize, Resize\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from sklearn.metrics import f1_score, accuracy_score, top_k_accuracy_score\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Parsing Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading metadata files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata = pd.read_csv(\"../../../resources/DF20/DanishFungi2020_train_metadata_DEV.csv\")\n",
    "test_metadata = pd.read_csv(\"../../../resources/DF20/DanishFungi2020_test_metadata_DEV.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metadata['image_path'] = test_metadata.apply(lambda x: '/local/nahouby/Datasets/DF20/' + x['image_path'].split('/SvampeAtlas-14.12.2020/')[-1], axis=1)\n",
    "test_metadata['image_path'] = test_metadata.apply(lambda x:  x['image_path'].split('.')[0] + '.JPG', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_metadata), len(test_metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.concat([train_metadata, test_metadata])\n",
    "len(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metadata.Habitat = test_metadata.Habitat.replace(np.nan, 'unknown', regex=True)\n",
    "test_metadata.Substrate = test_metadata.Substrate.replace(np.nan, 'unknown', regex=True)\n",
    "# test_metadata.month = test_metadata.month.replace(np.nan, 'unknown', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.Habitat = metadata.Habitat.replace(np.nan, 'unknown', regex=True)\n",
    "metadata.Substrate = metadata.Substrate.replace(np.nan, 'unknown', regex=True)\n",
    "# metadata.month = metadata.month.replace(np.nan, 0, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.Substrate.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_genus = np.zeros(len(metadata['class_id'].unique()))\n",
    "for species in metadata['class_id'].unique():\n",
    "    class_to_genus[species] = metadata[metadata['class_id'] == species]['genus_id'].unique()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Species distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_priors = np.zeros(len(metadata['class_id'].unique()))\n",
    "for species in metadata['class_id'].unique():\n",
    "    class_priors[species] = len(metadata[metadata['class_id'] == species])\n",
    "\n",
    "class_priors = class_priors/sum(class_priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting species-month distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_distributions = {}\n",
    "\n",
    "for _, observation in tqdm.tqdm(metadata.iterrows(), total=len(metadata)):\n",
    "    month = str(observation.month)\n",
    "    class_id = observation.class_id\n",
    "    if month not in month_distributions:        \n",
    "        month_distributions[month] = np.zeros(len(metadata['class_id'].unique()))\n",
    "    else:\n",
    "        month_distributions[month][class_id] += 1\n",
    "\n",
    "for key, value in month_distributions.items():\n",
    "    month_distributions[key] = value / sum(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting species-habitat distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "habitat_distributions = {}\n",
    "\n",
    "for _, observation in tqdm.tqdm(metadata.iterrows(), total=len(metadata)):\n",
    "    habitat = observation.Habitat\n",
    "    class_id = observation.class_id\n",
    "    if habitat not in habitat_distributions:        \n",
    "        habitat_distributions[habitat] = np.zeros(len(metadata['class_id'].unique()))\n",
    "    else:\n",
    "        habitat_distributions[habitat][class_id] += 1\n",
    "\n",
    "for key, value in habitat_distributions.items():\n",
    "    habitat_distributions[key] = value / sum(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting species-substrate distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substrate_distributions = {}\n",
    "\n",
    "for _, observation in tqdm.tqdm(metadata.iterrows(), total=len(metadata)):\n",
    "    substrate = observation.Substrate\n",
    "    class_id = observation.class_id\n",
    "    if substrate not in substrate_distributions:        \n",
    "        substrate_distributions[substrate] = np.zeros(len(metadata['class_id'].unique()))\n",
    "    else:\n",
    "        substrate_distributions[substrate][class_id] += 1\n",
    "\n",
    "for key, value in substrate_distributions.items():\n",
    "    substrate_distributions[key] = value / sum(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting with trained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=777):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "SEED = 777\n",
    "seed_torch(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        file_path = self.df['image_path'].values[idx]\n",
    "        label = self.df['class_id'].values[idx]\n",
    "        month = self.df['month'].values[idx]\n",
    "        sub = self.df['Substrate'].values[idx]\n",
    "        hab = self.df['Habitat'].values[idx]\n",
    "        \n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        return image, label, file_path, month, hab, sub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH, HEIGHT = 384, 384\n",
    "\n",
    "def get_transforms():\n",
    "\n",
    "    return Compose([Resize(WIDTH, HEIGHT),\n",
    "                    Normalize(mean = model_mean, std = model_std),\n",
    "                    ToTensorV2()])\n",
    "\n",
    "def getModel(architecture_name, target_size, pretrained = False):\n",
    "    net = timm.create_model(architecture_name, pretrained=pretrained)\n",
    "    net_cfg = net.default_cfg\n",
    "    last_layer = net_cfg['classifier']\n",
    "    num_ftrs = getattr(net, last_layer).in_features\n",
    "    setattr(net, last_layer, nn.Linear(num_ftrs, target_size))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = len(train_metadata['class_id'].unique())\n",
    "\n",
    "MODEL_NAME = 'vit_large_patch16_384'\n",
    "model = getModel(MODEL_NAME, N_CLASSES, pretrained=True)\n",
    "model_mean = list(model.default_cfg['mean'])\n",
    "model_std = list(model.default_cfg['std'])\n",
    "model.load_state_dict(torch.load('../../../checkpoints/DF20-ViT_large_patch16_384-100E.pth'))\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "test_dataset = TestDataset(test_metadata, transform=get_transforms())\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_val_loss = 0.\n",
    "preds = np.zeros((len(test_metadata)))\n",
    "GT_lbls = []\n",
    "image_paths = []\n",
    "preds_raw = []\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "months = []\n",
    "subs = []\n",
    "habitats = []\n",
    "\n",
    "for i, (images, labels, paths, M, H, S) in enumerate(tqdm.tqdm(test_loader, total=len(test_loader))):\n",
    "\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_preds = model(images)\n",
    "    preds[i * BATCH_SIZE: (i+1) * BATCH_SIZE] = y_preds.argmax(1).to('cpu').numpy()\n",
    "    GT_lbls.extend(labels.to('cpu').numpy())\n",
    "    preds_raw.extend(y_preds.to('cpu').numpy())\n",
    "    image_paths.extend(paths)\n",
    "    months.extend(M)\n",
    "    subs.extend(S)\n",
    "    habitats.extend(H)\n",
    "\n",
    "vanilla_f1 = f1_score(test_metadata['class_id'], preds, average='macro')\n",
    "vanilla_accuracy = accuracy_score(test_metadata['class_id'], preds)\n",
    "vanilla_recall_3 = top_k_accuracy_score(test_metadata['class_id'], preds_raw, k=3)\n",
    "vanilla_recall_5 = top_k_accuracy_score(test_metadata['class_id'], preds_raw, k=5)\n",
    "vanilla_recall_10 = top_k_accuracy_score(test_metadata['class_id'], preds_raw, k=10)\n",
    "\n",
    "print('Vanilla:', vanilla_f1, vanilla_accuracy, vanilla_recall_3, vanilla_recall_5, vanilla_recall_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting by Habitat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_predictions_H = []\n",
    "weighted_predictions_H = []\n",
    "weighted_predictions_raw_H = []\n",
    "prior_ratio_H = []\n",
    "\n",
    "for lbl, preds, hab in tqdm.tqdm(zip(GT_lbls, preds_raw, habitats), total=len(GT_lbls)):\n",
    "    \n",
    "    habitat_dist = habitat_distributions[hab]\n",
    "    preds = softmax(preds)\n",
    "    \n",
    "    p_habitat = (preds * habitat_dist) / sum(preds * habitat_dist)\n",
    "    prior_ratio = p_habitat / class_priors\n",
    "    max_index = np.argmax(prior_ratio * preds)        \n",
    "    \n",
    "    prior_ratio_H.append(prior_ratio)\n",
    "    weighted_predictions_raw_H.append(prior_ratio * preds)\n",
    "    weighted_predictions_H.append(max_index)\n",
    "    \n",
    "    if lbl != max_index:\n",
    "        wrong_predictions_H.append([lbl, hab])\n",
    "\n",
    "f1 = f1_score(test_metadata['class_id'], weighted_predictions_H, average='macro')\n",
    "accuracy = accuracy_score(test_metadata['class_id'], weighted_predictions_H)\n",
    "recall_3 = top_k_accuracy_score(test_metadata['class_id'], weighted_predictions_raw_H, k=3)\n",
    "print('Habitat:', f1, accuracy, recall_3)\n",
    "print('Habitat dif:', np.around(f1-vanilla_f1, 3), np.around((accuracy-vanilla_accuracy) * 100, 2), np.around((recall_3-vanilla_recall_3)*100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting by Substrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_predictions_S = []\n",
    "weighted_predictions_S = []\n",
    "weighted_predictions_raw_S = []\n",
    "prior_ratio_S = []\n",
    "\n",
    "for lbl, preds, sub in tqdm.tqdm(zip(GT_lbls, preds_raw, subs), total=len(GT_lbls)):\n",
    "\n",
    "    substrate_dist = substrate_distributions[sub]\n",
    "    preds = softmax(preds)\n",
    "    \n",
    "    p_substrate = (preds * substrate_dist) / sum(preds * substrate_dist)\n",
    "    prior_ratio = p_substrate / class_priors\n",
    "    max_index = np.argmax(prior_ratio * preds)     \n",
    "    \n",
    "    prior_ratio_S.append(prior_ratio)\n",
    "    weighted_predictions_raw_S.append(prior_ratio * preds)\n",
    "    weighted_predictions_S.append(max_index)\n",
    "    \n",
    "    if lbl != max_index:\n",
    "        wrong_predictions_S.append([lbl, sub])\n",
    "        \n",
    "f1 = f1_score(test_metadata['class_id'], weighted_predictions_S, average='macro')\n",
    "accuracy = accuracy_score(test_metadata['class_id'], weighted_predictions_S)\n",
    "recall_3 = top_k_accuracy_score(test_metadata['class_id'], weighted_predictions_raw_S, k=3)\n",
    "print('Substrate:', f1, accuracy, recall_3)\n",
    "print('Substrate dif:', np.around(f1-vanilla_f1, 3), np.around((accuracy-vanilla_accuracy) * 100, 2), np.around((recall_3-vanilla_recall_3)*100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting by Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_predictions_M = []\n",
    "weighted_predictions_M = []\n",
    "weighted_predictions_raw_M = []\n",
    "prior_ratio_M = []\n",
    "\n",
    "for lbl, preds, month in tqdm.tqdm(zip(GT_lbls, preds_raw, months), total=len(GT_lbls)):\n",
    "    \n",
    "    month_dist = month_distributions[str(float(month))]\n",
    "    preds = softmax(preds)\n",
    "    \n",
    "    p_month = (preds * month_dist) / sum(preds * month_dist)\n",
    "    prior_ratio = p_month / class_priors        \n",
    "    max_index = np.argmax(prior_ratio * preds)     \n",
    "    \n",
    "    prior_ratio_M.append(prior_ratio)\n",
    "    weighted_predictions_raw_M.append(prior_ratio * preds)\n",
    "    weighted_predictions_M.append(max_index)\n",
    "    \n",
    "    if lbl != max_index:\n",
    "        wrong_predictions_M.append([lbl, month])\n",
    "\n",
    "f1 = f1_score(test_metadata['class_id'], weighted_predictions_M, average='macro')\n",
    "accuracy = accuracy_score(test_metadata['class_id'], weighted_predictions_M)\n",
    "recall_3 = top_k_accuracy_score(test_metadata['class_id'], weighted_predictions_raw_M, k=3)\n",
    "print('Month:', f1, accuracy, recall_3)\n",
    "print('Month dif:', np.around(f1-vanilla_f1, 3), np.around((accuracy-vanilla_accuracy) * 100, 2), np.around((recall_3-vanilla_recall_3)*100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting by Month and Substrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "merged_predictions = []\n",
    "merged_predictions_raw = []\n",
    "\n",
    "for o, m, s, h in tqdm.tqdm(zip(preds_raw, prior_ratio_M, prior_ratio_S, prior_ratio_H), total=len(prior_ratio_M)):\n",
    "    \n",
    "    preds = softmax(preds)\n",
    "        \n",
    "    m_pred = (preds * m * s) / sum(preds * m * s)\n",
    "    max_index = np.argmax(m_pred)\n",
    "    \n",
    "    merged_predictions_raw.append(m_pred)\n",
    "    merged_predictions.append(max_index)\n",
    "    \n",
    "f1 = f1_score(test_metadata['class_id'], merged_predictions, average='macro')\n",
    "accuracy = accuracy_score(test_metadata['class_id'], merged_predictions)\n",
    "recall_3 = top_k_accuracy_score(test_metadata['class_id'], merged_predictions_raw, k=3)\n",
    "print('M+S:' , f1, accuracy, recall_3)\n",
    "print('M+S dif:', np.around(f1-vanilla_f1, 3), np.around((accuracy-vanilla_accuracy) * 100, 2), np.around((recall_3-vanilla_recall_3)*100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting by Month and Habitat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_predictions = []\n",
    "merged_predictions_raw = []\n",
    "\n",
    "for o, m, s, h in tqdm.tqdm(zip(preds_raw, prior_ratio_M, prior_ratio_S, prior_ratio_H), total=len(prior_ratio_M)):\n",
    "    \n",
    "    preds = softmax(preds)\n",
    "    \n",
    "    m_pred = (preds * m * h) / sum((preds * m * h))\n",
    "    max_index = np.argmax(m_pred)\n",
    "    \n",
    "    merged_predictions_raw.append(m_pred)    \n",
    "    merged_predictions.append(max_index)\n",
    "\n",
    "f1 = f1_score(test_metadata['class_id'], merged_predictions, average='macro')\n",
    "accuracy = accuracy_score(test_metadata['class_id'], merged_predictions)\n",
    "recall_3 = top_k_accuracy_score(test_metadata['class_id'], merged_predictions_raw, k=3)\n",
    "recall_5 = top_k_accuracy_score(test_metadata['class_id'], merged_predictions_raw, k=5)\n",
    "recall_10 = top_k_accuracy_score(test_metadata['class_id'], merged_predictions_raw, k=10)\n",
    "\n",
    "print('M+H:', f1, accuracy, recall_3, recall_5, recall_10)\n",
    "print('M+H dif:', np.around(f1-vanilla_f1, 3), np.around((accuracy-vanilla_accuracy) * 100, 2), np.around((recall_3-vanilla_recall_3)*100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting by Substrate and Habitat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_predictions = []\n",
    "merged_predictions_raw = []\n",
    "\n",
    "for o, m, s, h in tqdm.tqdm(zip(preds_raw, prior_ratio_M, prior_ratio_S, prior_ratio_H), total=len(prior_ratio_M)):\n",
    "    \n",
    "    preds = softmax(preds)\n",
    "    \n",
    "    m_pred = (preds * s * h) / sum((preds * s * h))\n",
    "    max_index = np.argmax(m_pred)\n",
    "    \n",
    "    merged_predictions_raw.append(m_pred)    \n",
    "    merged_predictions.append(max_index)\n",
    "\n",
    "f1 = f1_score(test_metadata['class_id'], merged_predictions, average='macro')\n",
    "accuracy = accuracy_score(test_metadata['class_id'], merged_predictions)\n",
    "recall_3 = top_k_accuracy_score(test_metadata['class_id'], merged_predictions_raw, k=3)\n",
    "\n",
    "print('S+H:' , f1, accuracy, recall_3)\n",
    "print('S+H dif:', np.around(f1-vanilla_f1, 3), np.around((accuracy-vanilla_accuracy) * 100, 2), np.around((recall_3-vanilla_recall_3)*100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting by Month, Substrate and Habitat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "wrong_predictions_all = []\n",
    "merged_predictions = []\n",
    "merged_predictions_raw = []\n",
    "\n",
    "wrong_predictions_all_genus = []\n",
    "merged_predictions_genus = []\n",
    "\n",
    "for lbl, img_path, o, m, s, h in tqdm.tqdm(zip(GT_lbls, image_paths, preds_raw, prior_ratio_M, prior_ratio_S, prior_ratio_H), total=len(prior_ratio_M)):\n",
    "    \n",
    "    preds = softmax(preds)\n",
    " \n",
    "    m_pred = (preds * m * s * h) / sum((preds * m * s * h))\n",
    "    max_index = np.argmax(m_pred)\n",
    "    \n",
    "    merged_predictions_raw.append(m_pred)    \n",
    "    merged_predictions.append(max_index)\n",
    "    \n",
    "    merged_predictions_genus.append(class_to_genus[max_index])\n",
    "    \n",
    "    if lbl != max_index:\n",
    "        wrong_predictions_all.append([lbl, max_index, img_path])\n",
    "    \n",
    "        if class_to_genus[lbl] != class_to_genus[max_index]:\n",
    "            wrong_predictions_all_genus.append([lbl, max_index, img_path])\n",
    "            \n",
    "f1 = f1_score(test_metadata['class_id'], merged_predictions, average='macro')\n",
    "accuracy = accuracy_score(test_metadata['class_id'], merged_predictions)\n",
    "recall_3 = top_k_accuracy_score(test_metadata['class_id'], merged_predictions_raw, k=3)\n",
    "recall_5 = top_k_accuracy_score(test_metadata['class_id'], merged_predictions_raw, k=5)\n",
    "recall_10 = top_k_accuracy_score(test_metadata['class_id'], merged_predictions_raw, k=10)\n",
    "\n",
    "print('All:', f1, accuracy, recall_3, recall_5, recall_10)\n",
    "print('All dif:', np.around(f1-vanilla_f1, 3), np.around((accuracy-vanilla_accuracy) * 100, 2), np.around((recall_3-vanilla_recall_3)*100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(test_metadata['genus_id'], merged_predictions_genus, average='macro')\n",
    "accuracy = accuracy_score(test_metadata['genus_id'], merged_predictions_genus)\n",
    "print('Genera lvl performance:', np.around(f1*100, 2), np.around(accuracy*100), 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
